{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBOOST model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinab/opt/anaconda3/envs/kaggle2/lib/python3.7/site-packages/pyparsing.py:3172: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile(self.reString)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Dec 2019\n",
    "\n",
    "@author: Dina Berenbaum\n",
    "\n",
    "XGBOOST model\n",
    "\"\"\"\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import UniformFloatHyperparameter, UniformIntegerHyperparameter\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.facade.smac_ac_facade import SMAC4AC\n",
    "\n",
    "\n",
    "class XgboostModel:\n",
    "    DEFAULT_PARAMS = {\"objective\": \"binary:logistic\", 'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 5, 'alpha': 10,\n",
    "                      'n_estimators': 10}\n",
    "    DEFAULT_OPTIMIZED_PARAMS = {\"objective\": \"binary:logistic\", 'colsample_bytree': 0.59, 'learning_rate': 0.1, 'max_depth': 8, 'alpha': 1,\n",
    "                                'n_estimators': 88}\n",
    "    DEFAULT_NFOLDS = 3\n",
    "    DEFAULT_BOOST_ROUNDS = 50\n",
    "    DEFAULT_EARLY_STOPPING = 10\n",
    "    DEFAULT_MATRIX = 'error'\n",
    "    DEFAULT_SEED = 123\n",
    "    DEFAULT_TEST_SIZE = 0.2\n",
    "\n",
    "    def __init__(self, X_train, Y_train, X_mytest, Y_mytest, optimized_params=True):\n",
    "        self._Xdata = X_train\n",
    "        self._ydata = Y_train\n",
    "        self._Xtest = X_mytest\n",
    "        self._ytest = Y_mytest\n",
    "        self._dmatrix = xgb.DMatrix(data=self._Xdata, label=self._ydata)\n",
    "        self._params = XgboostModel.DEFAULT_PARAMS\n",
    "        if optimized_params:\n",
    "            self._params = XgboostModel.DEFAULT_OPTIMIZED_PARAMS\n",
    "        self.xg_class = None\n",
    "        self.stats = None\n",
    "\n",
    "    def show_params(self):\n",
    "        for key, value in self._params:\n",
    "            print(key, value)\n",
    "\n",
    "    def train(self, model_path, do_hps=False, do_split=False, do_save=False, do_print=False, seed=42):\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if do_hps:\n",
    "            self.hyperparameter_search()\n",
    "\n",
    "        if do_print: print('Training...')\n",
    "\n",
    "        if do_split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self._Xdata, self._ydata, test_size=XgboostModel.DEFAULT_TEST_SIZE,\n",
    "                                                                random_state=XgboostModel.DEFAULT_SEED)\n",
    "        else:\n",
    "            X_train = self._Xdata\n",
    "            X_test = self._Xtest\n",
    "            y_train = self._ydata\n",
    "            y_test = self._ytest\n",
    "\n",
    "        self.xg_class = xgb.XGBClassifier(objective=self._params['objective'], colsample_bytree=self._params['colsample_bytree'],\n",
    "                                          learning_rate=self._params['learning_rate'], max_depth=self._params['max_depth'],\n",
    "                                          alpha=self._params['alpha'], n_estimators=self._params['n_estimators'],\n",
    "                                          random_state=seed)\n",
    "        self.xg_class.fit(X_train, y_train)\n",
    "\n",
    "        if do_print: print('Preforming prediction of the test data')\n",
    "        prediction = self.predict(X_test)\n",
    "        self.stats = self.test_statistics(y_test, prediction, print_it=do_print)\n",
    "        if do_save:\n",
    "            if do_print: print(f'Saving to {model_path + timestr}')\n",
    "            pickle.dump(self.xg_class, open(model_path + timestr + \".pickle.dat\", \"wb\"))\n",
    "\n",
    "    def predict(self, xdata):\n",
    "        predictions = self.xg_class.predict(xdata)\n",
    "        return predictions\n",
    "\n",
    "    def hyperparameter_search(self):\n",
    "        # Configuration space:\n",
    "        cs = ConfigurationSpace()\n",
    "        param_list = self._create_param_grid()\n",
    "        default_params = XgboostModel.DEFAULT_PARAMS.keys()\n",
    "        for param in param_list:\n",
    "            if param.name in default_params:\n",
    "                cs.add_hyperparameter(param)\n",
    "\n",
    "        # Scenario object:\n",
    "        scenario = Scenario({\"run_obj\": \"quality\",  # we optimize quality (alternatively runtime)\n",
    "                             \"runcount-limit\": 200,  # maximum function evaluations\n",
    "                             \"cs\": cs,  # configuration space\n",
    "                             \"deterministic\": \"true\"})\n",
    "\n",
    "        print(\"Optimizing! Depending on your machine, this might take a few minutes.\")\n",
    "        smac = SMAC4AC(scenario=scenario, rng=np.random.RandomState(42),\n",
    "                       tae_runner=self._kfold_train)\n",
    "        incumbent = smac.optimize()\n",
    "        inc_value = self._kfold_train(incumbent)\n",
    "        self._params = incumbent.get_dictionary()\n",
    "        self._params.update({\"objective\": \"binary:logistic\"})\n",
    "        print('The best configurations were updated into params and will be used in the training. Optimized value: %.2f' % inc_value)\n",
    "\n",
    "    def _kfold_train(self, cfg):\n",
    "        # K-fold training:\n",
    "        cfg = {k: cfg[k] for k in cfg if cfg[k]}\n",
    "        cv_results = xgb.cv(dtrain=self._dmatrix, params=cfg, nfold=XgboostModel.DEFAULT_NFOLDS,\n",
    "                            num_boost_round=XgboostModel.DEFAULT_BOOST_ROUNDS, early_stopping_rounds=XgboostModel.DEFAULT_EARLY_STOPPING,\n",
    "                            metrics=XgboostModel.DEFAULT_MATRIX, as_pandas=True, seed=XgboostModel.DEFAULT_SEED)\n",
    "        return cv_results[\"test-error-mean\"].tail(1).values[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_param_grid():\n",
    "        \"\"\"\n",
    "        Manually determined parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        param_list = [UniformFloatHyperparameter(\"colsample_bytree\", 0.1, 0.6),\n",
    "                      UniformFloatHyperparameter(\"learning_rate\", 0.05, 0.5),\n",
    "                      UniformIntegerHyperparameter(\"max_depth\", 2, 10),\n",
    "                      UniformIntegerHyperparameter(\"alpha\", 1, 50),\n",
    "                      UniformFloatHyperparameter(\"n_estimators\", 1, 100)]\n",
    "        return param_list\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_statistics(original_labels, predictions, print_it=False):\n",
    "        \"\"\"\n",
    "        Calculate and output test statistics of precision, recall and f1-score\n",
    "        :param original_labels:\n",
    "        :param predictions:\n",
    "        :param print_it:\n",
    "        :return: named tuple of the statistics results\n",
    "        \"\"\"\n",
    "        results = namedtuple('results', 'precision_score, recall_score, f1_score, accuaracy_score')\n",
    "\n",
    "        results.precision_score = precision_score(original_labels, predictions)\n",
    "        results.recall_score = recall_score(original_labels, predictions)\n",
    "        results.f1_score = f1_score(original_labels, predictions)\n",
    "        results.accuaracy_score = accuracy_score(original_labels, predictions)\n",
    "        if print_it:\n",
    "            print(f\"Precision: {results.precision_score}, Accuaracy: {results.accuaracy_score}, Recall: {results.recall_score}, \"\n",
    "                  f\"F1_score: {results.f1_score}\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
